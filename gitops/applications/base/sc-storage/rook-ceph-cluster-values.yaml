toolbox:
  enabled: true
monitoring:
  enabled: false
  interval: 60s

cephClusterSpec:
  dataDirHostPath: /var/lib/rook
  dashboard:
    enabled: true
    # prometheusEndpoint: http://central-monitoring-grafana-mimir-gateway.monitoring.svc.cluster.local/prometheus
    # prometheusEndpoint: http://prometheus-operated.monitoring.svc.cluster.local:9090
  network:
    provider: host
    connections:
      encryption:
        enabled: false
      compression:
        enabled: false

resources:
  storage:
    useAllNodes: true
    useAllDevices: true

cephBlockPools:
  - name: ceph-blockpool
    spec:
      failureDomain: host
      replicated:
        size: ${ARGOCD_ENV_rook_ceph_pool_replication_size}
    storageClass:
      enabled: true
      name: ceph-block
      isDefault: true
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      volumeBindingMode: "Immediate"
      parameters:
        imageFormat: "2"
        imageFeatures: layering
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: "${ARGOCD_ENV_storage_namespace}"
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: "${ARGOCD_ENV_storage_namespace}"
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
        csi.storage.k8s.io/node-stage-secret-namespace: "${ARGOCD_ENV_storage_namespace}"
        csi.storage.k8s.io/fstype: ext4

cephFileSystems:
  - name: ceph-filesystem
    spec:
      metadataPool:
        replicated:
          size: 3
      dataPools:
        - failureDomain: host
          replicated:
            size: ${ARGOCD_ENV_rook_ceph_pool_replication_size}
          name: data0
      metadataServer:
      activeCount: 1
      activeStandby: true
      priorityClassName: system-cluster-critical
    storageClass:
      enabled: true
      isDefault: false
      name: ceph-filesystem
      # (Optional) specify a data pool to use, must be the name of one of the data pools above, 'data0' by default
      pool: data0
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      volumeBindingMode: "Immediate"
      parameters:
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: "${ARGOCD_ENV_storage_namespace}"
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: "${ARGOCD_ENV_storage_namespace}"
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
        csi.storage.k8s.io/node-stage-secret-namespace: "${ARGOCD_ENV_storage_namespace}"
        csi.storage.k8s.io/fstype: ext4

cephObjectStores:
  - name: ceph-objectstore
    spec:
      metadataPool:
        failureDomain: host
        replicated:
          size: 3
      dataPool:
        failureDomain: host
        replicated:
          size: ${ARGOCD_ENV_rook_ceph_pool_replication_size}
        # erasureCoded:
        #   dataChunks: 2
        #   codingChunks: 1
        parameters:
          bulk: "true"
      preservePoolsOnDelete: true
      gateway:
        hostNetwork: false
        port: 80
        resources:
          limits:
            memory: "2Gi"
        #          requests:
        #            cpu: "1000m"
        #            memory: "1Gi"
        #securePort: 443
        #sslCertificateRef: objectstore-internal-tls
        instances: 1
        priorityClassName: system-cluster-critical
    storageClass:
      enabled: true
      name: ceph-bucket
      reclaimPolicy: Delete
      volumeBindingMode: "Immediate"
      annotations: {}
      labels: {}
      # see https://github.com/rook/rook/blob/master/Documentation/Storage-Configuration/Object-Storage-RGW/ceph-object-bucket-claim.md#storageclass for available configuration
      parameters:
        # note: objectStoreNamespace and objectStoreName are configured by the chart
        region: us-east-1
    ingress:
      enabled: false
