apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: ${ARGOCD_ENV_rook_ceph_namespace}
spec:
  cephVersion:
    image: quay.io/ceph/ceph:${ARGOCD_ENV_ceph_image_version}
    allowUnsupported: false
  dataDirHostPath: /var/lib/rook
  monitoring:
    enabled: true
  dashboard:
    enabled: true
  mgr:
    count: 2
    allowMultiplePerNode: false
  mon:
    count: 3
    allowMultiplePerNode: false
    volumeClaimTemplate:
      spec:
        storageClassName: "${ARGOCD_ENV_ceph_osd_volumes_storage_class}-csi"
        resources:
          requests:
            storage: "${ARGOCD_ENV_ceph_mon_volume_size}"
  storage:
    allowDeviceClassUpdate: false # whether to allow changing the device class of an OSD after it is created
    allowOsdCrushWeightUpdate: true # whether to allow resizing the OSD crush weight after osd pvc is increased
    storageClassDeviceSets:
      - name: set1
        # The number of OSDs to create from this device set
        count: ${ARGOCD_ENV_ceph_osd_count}
        portable: true
        tuneDeviceClass: true
        tuneFastDeviceClass: false
        encrypted: false
        placement:
          topologySpreadConstraints:
            - maxSkew: 1
              topologyKey: kubernetes.io/hostname
              whenUnsatisfiable: ScheduleAnyway
              labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - rook-ceph-osd
        preparePlacement:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                      - key: app
                        operator: In
                        values:
                          - rook-ceph-osd
                      - key: app
                        operator: In
                        values:
                          - rook-ceph-osd-prepare
                  topologyKey: kubernetes.io/hostname
          topologySpreadConstraints:
            - maxSkew: 1
              # IMPORTANT: If you don't have zone labels, change this to another key such as kubernetes.io/hostname
              topologyKey: topology.kubernetes.io/zone
              whenUnsatisfiable: DoNotSchedule
              labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - rook-ceph-osd-prepare
        #resources:
        # These are the OSD daemon limits. For OSD prepare limits, see the separate section below for "prepareosd" resources
        #   limits:
        #     memory: "4Gi"
        #   requests:
        #     cpu: "500m"
        #     memory: "4Gi"
        volumeClaimTemplates:
          - metadata:
              name: data
            spec:
              resources:
                requests:
                  storage: "${ARGOCD_ENV_ceph_volume_size_per_osd}"
              storageClassName: "${ARGOCD_ENV_ceph_osd_volumes_storage_class}-csi"
              volumeMode: Block
              accessModes:
                - ReadWriteOnce
    onlyApplyOSDPlacement: false
  #resources:
  #  prepareosd:
  #    requests:
  #      cpu: "200m"
  #      memory: "200Mi"
  priorityClassNames:
    # If there are multiple nodes available in a failure domain (e.g. zones), the
    # mons and osds can be portable and set the system-cluster-critical priority class.
    mon: system-node-critical
    osd: system-node-critical
    mgr: system-cluster-critical
  disruptionManagement:
    managePodBudgets: true
    osdMaintenanceTimeout: 30
    pgHealthCheckTimeout: 0
