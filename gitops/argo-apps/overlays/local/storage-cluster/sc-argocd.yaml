apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: argocd
  namespace: argocd
  annotations:
    argocd.argoproj.io/sync-wave: '-20'
    argocd.argoproj.io/compare-options: ServerSideDiff=false
    # argocd.argoproj.io/sync-options: Prune=false,Replace=false

  # Add this finalizer ONLY if you want these to cascade delete (A cascade delete, deletes both the app and its resources, rather than only the app.)
  # finalizers:
  #   - resources-finalizer.argocd.argoproj.io

spec:
  project: default
  ignoreDifferences:
    - group: ""
      name: "argocd-rbac-cm"
      kind: ConfigMap
      jsonPointers:
        - /data/policy.csv
    - group: ""
      name: "argocd-cm"
      kind: ConfigMap
      jsonPointers:
        - /data/oidc.config
  syncPolicy:
    syncOptions:
      - CreateNamespace=true
      - PrunePropagationPolicy=foreground
      - RespectIgnoreDifferences=true

    automated:
      prune: false
      selfHeal: false

  destination:
    server: "https://kubernetes.default.svc"
    namespace: argocd

  sources:
    - chart: argo-cd
      repoURL: https://argoproj.github.io/argo-helm
      targetRevision: 8.0.1

      helm:
        releaseName: argocd
        valuesObject:
          fullnameOverride: argocd
          crds:
            install: true
          global:
            logging:
              format: json

          applicationSet:
            metrics:
              enabled: false
          notifications:
            enabled: false
          configs:
            secret:
              createSecret: false
            cm:
              exec.enabled: "true"
              kustomize.buildOptions: "--load-restrictor LoadRestrictionsNone"
              # DO NOT USE in production, this is only used to improve reconciliation in testing env.
              timeout.reconciliation: "10s"
              application.resourceTrackingMethod: annotation
              #url: https://argocd-sc.int.ccinfra04.drpp-onprem.global
              resource.exclusions: |
                - apiGroups:
                  - "*"
                  kinds:
                  - ProviderConfigUsage
              resource.customizations: |
                argoproj.io/Application:
                  health.lua: |
                    hs = {}
                    hs.status = "Progressing"
                    hs.message = ""
                    if obj.status ~= nil then
                      local status = obj.status
                      if status.conditions ~= nil then
                        for i, condition in ipairs(status.conditions) do
                          if condition.type ~= nil and string.match(condition.type, '.*Error$') then
                            hs.status = "Degraded"
                            hs.message = condition.message
                            return hs
                          end
                        end
                      end
                      if status.health ~= nil then
                        local health = status.health
                        hs.status = health.status
                        if health.message ~= nil then
                          hs.message = health.message
                        end
                        local syncStatus = (status.sync and status.sync.status or nil)
                        if hs.status == "Healthy" and syncStatus ~= "Synced" then
                          hs.status = "Progressing"
                        end
                      end
                    end
                    return hs
                cert-manager.io/ClusterIssuer:
                  health.lua: |
                    local hs = {}
                    if obj.status ~= nil then
                      if obj.status.conditions ~= nil then
                        for i, condition in ipairs(obj.status.conditions) do
                          if condition.type == "Ready" and condition.status == "False" then
                            hs.status = "Degraded"
                            hs.message = condition.message
                            return hs
                          end
                          if condition.type == "Ready" and condition.status == "True" then
                            hs.status = "Healthy"
                            hs.message = condition.message
                            return hs
                          end
                        end
                      end
                    end

                    hs.status = "Progressing"
                    hs.message = "Initializing ClusterIssuer"
                    return hs
                cert-manager.io/Certificate:
                  health.lua: |
                    local hs = {}
                    if obj.status ~= nil then
                      if obj.status.conditions ~= nil then

                        -- Always Handle Issuing First to ensure consistent behaviour
                        for i, condition in ipairs(obj.status.conditions) do
                          if condition.type == "Issuing" and condition.status == "True" then
                            hs.status = "Progressing"
                            hs.message = condition.message
                            return hs
                          end
                        end

                        for i, condition in ipairs(obj.status.conditions) do
                          if condition.type == "Ready" and condition.status == "False" then
                            hs.status = "Degraded"
                            hs.message = condition.message
                            return hs
                          end
                          if condition.type == "Ready" and condition.status == "True" then
                            hs.status = "Healthy"
                            hs.message = condition.message
                            return hs
                          end
                        end
                      end
                    end

                    hs.status = "Progressing"
                    hs.message = "Waiting for certificate"
                    return hs

                ceph.rook.io/CephBlockPool:
                  health.lua: |
                    hs = {}
                    if obj.status ~= nil then
                      if obj.status.phase == "Ready" then
                        hs.status = "Healthy"
                        hs.message = "CephBlockPool Ready"
                        return hs
                      end
                    end
                    hs.status = "Progressing"
                    hs.message = "Waiting for CephBlockPool"
                    return hs

                # TODO: enable again after fixing the warning on ceph cluster object
                ceph.rook.io/CephCluster:
                  health.lua: |
                    hs = {}
                    if obj.status ~= nil and obj.status.ceph ~= nil and obj.status.ceph.health ~= nil then
                      local health = obj.status.ceph.health
                      if health == "HEALTH_OK" then
                        hs.status = "Healthy"
                        hs.message = "Ceph cluster is healthy (HEALTH_OK)"
                      elseif health == "HEALTH_WARN" then
                        hs.status = "Degraded"
                        hs.message = "Ceph cluster warning: " .. health
                      else
                        hs.status = "Degraded"
                        hs.message = "Ceph cluster not healthy: " .. health
                      end
                    else
                      hs.status = "Progressing"
                      hs.message = "Waiting for Ceph cluster status..."
                    end
                    return hs

                ceph.rook.io/CephObjectStore:
                  health.lua: |
                    hs = {}
                    if obj.status ~= nil then
                      if obj.status.phase == "Ready" then
                        hs.status = "Healthy"
                        hs.message = "CephObjectStore Ready"
                        return hs
                      end
                    end
                    hs.status = "Progressing"
                    hs.message = "Waiting for CephObjectStore"
                    return hs

            # rbac:
            #   policy.default: ''
            #   create: true
            params:
              server.insecure: true
              # Mandatory for extensions to work
              server.enable.proxy.extension: "true"
              reposerver.enable.git.submodule: "false"
              applicationsetcontroller.enable.git.submodule: "false"

              #Enable Server-Side Diff so argocd play nicely with Kyverno mutating webhooks:
              #https://argo-cd.readthedocs.io/en/stable/user-guide/diff-strategies/#mutation-webhooks
              controller.diff.server.side: "true"

            cmp:
              create: true
              plugins:
                envsubstappofapp:
                  init:
                    command: ["sh", "-c"]
                    args:
                      [
                        "kustomize build . --load-restrictor LoadRestrictionsNone -o raw-kustomization.yaml",
                      ]
                  generate:
                    command: ["sh", "-c"]
                    args:
                      [
                        "envsubst < raw-kustomization.yaml > processed-kustomization.yaml && cp processed-kustomization.yaml /dev/stdout",
                      ]
                  discover:
                    fileName: "kustomization.*"
                envsubst:
                  discover:
                    fileName: "kustomization.*"
                  generate:
                    command: ["sh", "-c"]
                    args:
                      [
                        "for f in *.yaml ; do cat $f | envsubst > $f.sub && mv $f.sub $f ; done && kustomize build . --enable-helm --helm-kube-version 1.31-0 --load-restrictor LoadRestrictionsNone > /dev/stdout",
                      ]

                helmfile:
                  discover:
                    fileName: "helmfile.yaml"
                  generate:
                    command: ["sh", "-c"]
                    args:
                      [
                        "for f in *.yaml ; do cat $f | envsubst > $f.sub && sed -i -e 's/ref+/reff+/g' $f.sub && mv $f.sub $f ; done && helmfile template --quiet -f helmfile.yaml > helmfileout.yaml && sed -i -e 's/reff+/ref+/g' helmfileout.yaml && cp helmfileout.yaml /dev/stdout",
                      ]

          ## Controller ##
          controller:
            # resources:
            #   limits:
            #     cpu: 1
            #     memory: 1024Mi
            #   requests:
            #     cpu: 500m
            #     memory: 1024Mi
            metrics:
              enabled: false
              serviceMonitor:
                enabled: false
                namespace: argocd
                additionalLabels:
                  prometheus.io/scrap-with: kube-prometheus-stack

          ## DEX ##
          dex:
            enabled: false
            metrics:
              enabled: false
              serviceMonitor:
                enabled: false # enable for production
                namespace: argocd
                additionalLabels:
                  prometheus.io/scrap-with: kube-prometheus-stack

          ## RepoServer ##
          repoServer:
            # resources:
            #   limits:
            #     cpu: 500m
            #     memory: 1.5Gi
            #   requests:
            #     cpu: 250m
            #     memory: 512Mi
            env:
              - name: HELM_CACHE_HOME
                value: /helm-working-dir
              - name: HELM_CONFIG_HOME
                value: /helm-working-dir
              - name: HELM_DATA_HOME
                value: /helm-working-dir

            volumes:
              - name: custom-tools
                emptyDir: {}
              - name: cmp-plugin
                configMap:
                  name: argocd-cmp-cm

            initContainers:
              - name: helm-plugins
                image: alpine/helm:3.14.0
                volumeMounts:
                  - name: helm-working-dir
                    mountPath: /helm-working-dir
                env:
                  - name: HELM_CACHE_HOME
                    value: /helm-working-dir
                  - name: HELM_CONFIG_HOME
                    value: /helm-working-dir
                  - name: HELM_DATA_HOME
                    value: /helm-working-dir
                command: ["/bin/sh", "-c"]
                args:
                  - helm plugin install https://github.com/aslafy-z/helm-git --version 1.3.0 && rm -rf /helm-working-dir/plugins/https* && chmod -R 777 $HELM_DATA_HOME

              - name: download-tools
                image: golang:1.22.4-alpine3.20
                command: [sh, -c]
                args:
                  - apk add git && go install github.com/drone/envsubst/cmd/envsubst@v1.0.3 && mv $GOPATH/bin/envsubst /custom-tools/ && wget -qO- https://github.com/helmfile/helmfile/releases/download/v1.0.0/helmfile_1.0.0_linux_amd64.tar.gz | tar xvz helmfile && mv helmfile /custom-tools/
                volumeMounts:
                  - mountPath: /custom-tools
                    name: custom-tools

            extraContainers:
              - name: debug-tools
                image: quay.io/argoproj/argocd
                command: [sh, -c]
                args:
                  - while true; do echo "running"; sleep 300; done
                volumeMounts:
                  - mountPath: /var/run/argocd
                    name: var-files
                  - mountPath: /home/argocd/cmp-server/plugins
                    name: plugins
                  - mountPath: /tmp
                    name: tmp

                  # Important: Mount tools into $PATH
                  - name: custom-tools
                    subPath: envsubst
                    mountPath: /usr/local/bin/envsubst
                  - name: custom-tools
                    subPath: helmfile
                    mountPath: /usr/local/bin/helmfile

                  - name: helm-working-dir
                    mountPath: /helm-working-dir

              - name: envsubstappofapp
                command: [/var/run/argocd/argocd-cmp-server]
                image: quay.io/argoproj/argocd
                args: [--loglevel, debug]
                securityContext:
                  runAsNonRoot: true
                  runAsUser: 999
                volumeMounts:
                  - mountPath: /var/run/argocd
                    name: var-files
                  - mountPath: /home/argocd/cmp-server/plugins
                    name: plugins
                  - mountPath: /tmp
                    name: tmp

                  # Register plugins into sidecar
                  - mountPath: /home/argocd/cmp-server/config/plugin.yaml
                    subPath: envsubstappofapp.yaml
                    name: cmp-plugin

                  # Important: Mount tools into $PATH
                  - name: custom-tools
                    subPath: envsubst
                    mountPath: /usr/local/bin/envsubst

              - name: envsubst
                command: [/var/run/argocd/argocd-cmp-server]
                image: quay.io/argoproj/argocd
                args: [--loglevel, debug]
                securityContext:
                  runAsNonRoot: true
                  runAsUser: 999
                volumeMounts:
                  - mountPath: /var/run/argocd
                    name: var-files
                  - mountPath: /home/argocd/cmp-server/plugins
                    name: plugins
                  - mountPath: /tmp
                    name: tmp

                  # Register plugins into sidecar
                  - mountPath: /home/argocd/cmp-server/config/plugin.yaml
                    subPath: envsubst.yaml
                    name: cmp-plugin

                  # Important: Mount tools into $PATH
                  - name: custom-tools
                    subPath: envsubst
                    mountPath: /usr/local/bin/envsubst

              - name: helmfile
                command: [/var/run/argocd/argocd-cmp-server]
                image: quay.io/argoproj/argocd
                args: [--loglevel, debug]
                securityContext:
                  runAsNonRoot: true
                  runAsUser: 999
                env:
                  - name: HELM_CACHE_HOME
                    value: /helm-working-dir
                  - name: HELM_CONFIG_HOME
                    value: /helm-working-dir
                  - name: HELM_DATA_HOME
                    value: /helm-working-dir
                volumeMounts:
                  - mountPath: /var/run/argocd
                    name: var-files
                  - mountPath: /home/argocd/cmp-server/plugins
                    name: plugins
                  - mountPath: /tmp
                    name: tmp

                  # Register plugins into sidecar
                  - mountPath: /home/argocd/cmp-server/config/plugin.yaml
                    subPath: helmfile.yaml
                    name: cmp-plugin

                  # Important: Mount tools into $PATH
                  - name: custom-tools
                    subPath: helmfile
                    mountPath: /usr/local/bin/helmfile

                  - name: custom-tools
                    subPath: envsubst
                    mountPath: /usr/local/bin/envsubst

                  - name: helm-working-dir
                    mountPath: /helm-working-dir

            metrics:
              enabled: false
              serviceMonitor:
                enabled: false
                namespace: argocd
                additionalLabels:
                  prometheus.io/scrap-with: kube-prometheus-stack

          ## Server ##
          server:
            # resources:
            #   limits:
            #     cpu: 400m
            #     memory: 512Mi
            #   requests:
            #     cpu: 400m
            #     memory: 512Mi
            metrics:
              enabled: false
              serviceMonitor:
                enabled: false
                additionalLabels:
                  prometheus.io/scrap-with: kube-prometheus-stack
                namespace: argocd
            ingress:
              enabled: false

            extensions:
              enabled: true
              extensionList:
                # - name: metrics
                #   env:
                #     - name: EXTENSION_URL
                #       value: https://github.com/argoproj-labs/argocd-extension-metrics/releases/download/v1.0.3/extension.tar.gz
                #     - name: EXTENSION_CHECKSUM_URL
                #       value: https://github.com/argoproj-labs/argocd-extension-metrics/releases/download/v1.0.3/extension_checksums.txt
                - name: rollout
                  env:
                    - name: EXTENSION_URL
                      value: https://github.com/argoproj-labs/rollout-extension/releases/download/v0.3.7/extension.tar

              resources:
                {}
                # limits:
                #   cpu: 50m
                #   memory: 128Mi
                # requests:
                #   cpu: 10m
                #   memory: 64Mi

          ## Redis ##
          redis:
            resources:
              limits:
                cpu: 120m
                memory: 256Mi
              requests:
                cpu: 120m
                memory: 256Mi
            metrics:
              enabled: false
              serviceMonitor:
                enabled: false
                additionalLabels:
                  prometheus.io/scrap-with: kube-prometheus-stack
                namespace: argocd
